{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d350d4-6ced-42f4-b250-4e597d379f7c",
   "metadata": {},
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d2e52b9-8370-41fa-9f05-45a1e4deaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindnlp\n",
    "import mindspore\n",
    "from mindnlp import core\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4695182d-bfa6-4c07-a631-36f38222ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c9e20-f52d-4099-89dc-3e5b7b1a9329",
   "metadata": {},
   "source": [
    "# æ•°æ®é›†é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1f8d4c-35e3-4684-8888-4c8dc76f02a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…±å¤„ç† 3752 æ¡æ•°æ®ï¼Œå·²ä¿å­˜åˆ° train.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def process_classification(path):\n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    processed = []\n",
    "    for item in data:\n",
    "        # æ„é€  instruction, input, output\n",
    "        instruction = \"è¯·é˜…è¯»ä»¥ä¸‹è¯„è®ºï¼Œå¹¶åˆ¤æ–­æœ€åä¸€æ¡è¯„è®ºæ˜¯å¦æ˜¯è®½åˆºæ€§çš„ï¼Œå¹¶ç»™å‡ºè®½åˆºç±»å‹ã€‚\"\n",
    "        input_text = \"\\n\".join(item[\"comments\"])\n",
    "        output_text = f\"è¿™æ˜¯è®½åˆºï¼Œç±»å‹æ˜¯ {item['classification']}ã€‚è§£æï¼š{item['explanation']}\"\n",
    "        processed.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        })\n",
    "    return processed\n",
    "\n",
    "def process_understanding(path):\n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    processed = []\n",
    "    for item in data:\n",
    "        instruction = f\"è¯·å›ç­”å…³äºä»¥ä¸‹è¯„è®ºçš„é—®é¢˜ï¼š{item['question']}\"\n",
    "        input_text = \"\\n\".join(item[\"comments\"])\n",
    "        output_text = item[\"qa analysis\"]\n",
    "        processed.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        })\n",
    "    return processed\n",
    "\n",
    "def process_roleplay(path):\n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    processed = []\n",
    "    for item in data:\n",
    "        input_text = \"\\n\".join(item[\"comments\"])\n",
    "        instruction = \"è¯·æ¨¡ä»¿äººç±»è¯„è®ºè€…çš„é£æ ¼ï¼Œç”Ÿæˆä¸€æ¡è®½åˆºæ€§è¯„è®ºã€‚\"\n",
    "        # åˆå¹¶è§’è‰² A~D çš„è®½åˆºè¯„è®ºä½œä¸ºè¾“å‡ºå€™é€‰\n",
    "        roleplay = item.get(\"roleplay\", {})\n",
    "        outputs = [v for k,v in roleplay.items() if k in [\"A\",\"B\",\"C\",\"D\"] and v]\n",
    "        if not outputs:\n",
    "            continue\n",
    "        output_text = \"å¯èƒ½çš„è®½åˆºè¯„è®ºç¤ºä¾‹ï¼š\\n\" + \"\\n\".join(outputs)\n",
    "        processed.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        })\n",
    "    return processed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è·¯å¾„æ›¿æ¢æˆä½ è‡ªå·±çš„æ–‡ä»¶è·¯å¾„\n",
    "    cls_data = process_classification(\"classification.json\")\n",
    "    und_data = process_understanding(\"understanding.json\")\n",
    "    rp_data  = process_roleplay(\"roleplay.json\")\n",
    "\n",
    "    all_data = cls_data + und_data + rp_data\n",
    "\n",
    "    with open(\"train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"å…±å¤„ç† {len(all_data)} æ¡æ•°æ®ï¼Œå·²ä¿å­˜åˆ° train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f967e4-2d98-4502-b66d-868a56e8c206",
   "metadata": {},
   "source": [
    "# æ•°æ®åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2dcffaf-6171-4836-8f89-fe46ae72f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 2: æ•°æ®åŠ è½½\n",
    "# -------------------------\n",
    "df = pd.read_json(\"./train.json\")\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4640013-fd98-47a9-90b6-3380b6886f39",
   "metadata": {},
   "source": [
    "# å®ä¾‹åŒ–åˆ†è¯å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d549f3-64d3-4db0-82b5-85ef1c920bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='./DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<ï½œbeginâ–ofâ–sentenceï½œ>', 'eos_token': '<ï½œendâ–ofâ–sentenceï½œ>', 'pad_token': '<ï½œendâ–ofâ–sentenceï½œ>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<ï½œendâ–ofâ–sentenceï½œ>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<ï½œUserï½œ>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151645: AddedToken(\"<ï½œAssistantï½œ>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151646: AddedToken(\"<ï½œbeginâ–ofâ–sentenceï½œ>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å®ä¾‹åŒ–tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', \n",
    "                                          use_fast=False,\n",
    "                                          trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8976fd-8ae4-4801-8fe2-893fb71b7297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3752/3752 [00:05<00:00, 635.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªè®½åˆºè¯„è®ºç”ŸæˆAI<|im_end|>\\n<|im_start|>user\\n{example['instruction']}\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9757a9d9-8d66-4f9f-91e3-e3bd1527056f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªè®½åˆºè¯„è®ºç”ŸæˆAI<|im_end|>\\n<|im_start|>user\\nè¯·é˜…è¯»ä»¥ä¸‹è¯„è®ºï¼Œå¹¶åˆ¤æ–­æœ€åä¸€æ¡è¯„è®ºæ˜¯å¦æ˜¯è®½åˆºæ€§çš„ï¼Œå¹¶ç»™å‡ºè®½åˆºç±»å‹ã€‚\\næ°´é‡Œæ ¹æœ¬æ²¡æœ‰é±¼ï¼Œä¸ç„¶æˆ‘ä¸ºä»€ä¹ˆé’“ä¸åˆ°ï¼Ÿ\\né’“é±¼ä½¬æ°¸ä¸ç©ºå†›ï¼ï¼ï¼[å¤§ç¬‘]\\nä½ æ˜¯å¯¹çš„\\nå”¯å¿ƒä¸»ä¹‰è€…\\né‚£å¾ˆæœ‰ç”Ÿæ´»äº†\\né’“ä¸åˆ°æ€ä¹ˆä¸æ‰¾æ‰¾è‡ªå·±çš„é—®é¢˜ï¼Ÿ\\nèœå¸‚åœºæœ‰\\nä½ æ˜¯å¯¹çš„\\nä½ æ˜¯å¯¹çš„\\nåº”è¯¥æ˜¯æ‰“çªæ²¡æ‰“å¥½ï¼Œè¦ä¸å†è¯•è¯•ï¼Ÿ[doge]\\nä½ å¾—é€šè¿‡å®éªŒå»è¯æ˜å•Šï¼ŒæŠ½æ°´å°±æ˜¯ä¸ªå¥½åŠæ³•[doge_é‡‘ç®]<|im_end|>\\n<|im_start|>assistant\\nè¿™æ˜¯è®½åˆºï¼Œç±»å‹æ˜¯ factã€‚è§£æï¼šè¯¥ç»„è¯„è®ºè®¨è®ºçš„è¯é¢˜ä¸ºé’“é±¼çš„é—®é¢˜ï¼Œå…·ä½“æ˜¯å›´ç»•â€œæ°´é‡Œæœ‰æ²¡æœ‰é±¼â€ä»¥åŠâ€œä¸ºä»€ä¹ˆé’“ä¸åˆ°é±¼â€å±•å¼€çš„ã€‚æœ€åä¸€æ¡è¯„è®ºè®½åˆºçš„å¯¹è±¡æ˜¯æœ€åˆæå‡ºâ€œæ°´é‡Œæ ¹æœ¬æ²¡æœ‰é±¼ï¼Œä¸ç„¶æˆ‘ä¸ºä»€ä¹ˆé’“ä¸åˆ°ï¼Ÿâ€è¿™ä¸€è§‚ç‚¹çš„äººã€‚è¯¥è¯„è®ºé€šè¿‡äº‹å®è®½åˆº(fact)çš„æ–¹å¼ï¼Œè¡¨é¢ä¸Šçœ‹ä¼¼è®¤çœŸåœ°å»ºè®®é€šè¿‡â€œæŠ½æ°´â€æ¥éªŒè¯æ°´é‡Œæ˜¯å¦æœ‰é±¼ï¼Œä½†å®é™…ä¸Šè¿™ç§åšæ³•æ˜¯éå¸¸ä¸åˆ‡å®é™…ä¸”è’è°¬çš„ã€‚è¯„è®ºè€…åˆ©ç”¨è¿™ç§æç«¯çš„æ–¹å¼æ¥è®½åˆºåŸè§‚ç‚¹çš„é€»è¾‘é—®é¢˜ï¼šä»…å‡­è‡ªå·±é’“ä¸åˆ°é±¼å°±æ–­å®šæ°´é‡Œæ²¡æœ‰é±¼ï¼Œæ˜¾ç„¶æ˜¯ç«™ä¸ä½è„šçš„ã€‚é€šè¿‡æå‡ºä¸€ä¸ªæ˜æ˜¾ä¸åˆç†ä½†ç†è®ºä¸Šå¯è¡Œçš„å®éªŒæ–¹æ³•ï¼ˆæŠ½æ°´ï¼‰ï¼Œè¯„è®ºè€…çªå‡ºäº†åŸè§‚ç‚¹çš„è’è°¬æ€§ï¼Œå¹¶æš—ç¤ºå…¶æ¨ç†è¿‡äºç®€å•å’Œç‰‡é¢ï¼Œä»è€Œè¾¾åˆ°è®½åˆºæ•ˆæœã€‚<ï½œendâ–ofâ–sentenceï½œ>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_id[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec142f-4cfe-498d-8f17-6979457e8bfd",
   "metadata": {},
   "source": [
    "# æ¨¡å‹åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d76f2-76e9-4f23-8822-f662246459af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained('deepseek-ai//DeepSeek-R1-Distill-Qwen-1.5B',\n",
    "                                             ms_dtype=mindspore.bfloat16, device_map=0)\n",
    "\n",
    "# å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b773ef3-5dd4-490f-9138-8422c87197ed",
   "metadata": {},
   "source": [
    "# å¾®è°ƒå‰æ¨ç†\n",
    "è¿›è¡Œæ¨¡å‹æ¨ç†æŸ¥çœ‹æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2df7c7ac-9db1-4fab-875e-324c6afb52e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾®è°ƒå‰æ¨ç†ç»“æœï¼š\n",
      "å—¯ï¼Œç”¨æˆ·ç»™äº†ä¸€ä¸ªè¯„è®ºï¼šâ€œæ°´é‡Œæ ¹æœ¬æ²¡æœ‰é±¼ï¼Œä¸ç„¶æˆ‘ä¸ºä»€ä¹ˆé’“ä¸åˆ°ï¼Ÿâ€ç„¶åè®©æˆ‘ç”Ÿæˆä¸€æ¡è®½åˆºè¯„è®ºã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£è¿™ä¸ªè¯„è®ºçš„æ„æ€ã€‚ç”¨æˆ·æ˜¯åœ¨è¯´ï¼Œå¦‚æœæ°´é‡ŒçœŸçš„æœ‰é±¼ï¼Œä¸ºä»€ä¹ˆä»–é’“ä¸åˆ°ï¼Œå¯èƒ½æ˜¯å› ä¸ºé±¼å¤ªå°æˆ–è€…æ°´å¤ªæ·±ï¼Œæˆ–è€…ä»–é’“çš„çº¿å¤ªçŸ­ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘å¾—è€ƒè™‘å¦‚ä½•ç”¨è®½åˆºçš„æ–¹å¼å›åº”ã€‚è®½åˆºé€šå¸¸æ˜¯æŒ‡å¸¦æœ‰è´¬ä¹‰æˆ–æš—ç¤ºé”™è¯¯çš„è¯„è®ºï¼Œç”¨æ¥æŒ‡å‡ºé—®é¢˜æ‰€åœ¨ã€‚æ‰€ä»¥ï¼Œæˆ‘éœ€è¦æ‰¾åˆ°ä¸€ä¸ªåé—®çš„æ–¹å¼ï¼Œæˆ–è€…æŒ‡å‡ºåŸå› ï¼Œä½†å¸¦æœ‰è®½åˆºçš„è¯­æ°”ã€‚\n",
      "\n",
      "å¯èƒ½çš„æ€è·¯æ˜¯ï¼šâ€œä½ ä¸ºä»€ä¹ˆé’“ä¸åˆ°ï¼Ÿéš¾é“æ°´é‡Œæ²¡æœ‰é±¼å—ï¼Ÿâ€ è¿™æ ·ç›´æ¥åé—®ï¼Œå¯èƒ½æ˜¾å¾—æœ‰äº›è®½åˆºï¼Œå› ä¸ºå¦‚æœæ°´é‡Œæœ‰é±¼ï¼Œä¸ºä»€ä¹ˆé’“ä¸åˆ°ã€‚ä½†å¯èƒ½ç”¨æˆ·å¸Œæœ›æ›´æ·±å…¥ä¸€ç‚¹ï¼ŒæŒ‡å‡ºåŸå› ã€‚\n",
      "\n",
      "å¦ä¸€ç§å¯èƒ½æ˜¯ï¼šâ€œä½ é’“ä¸åˆ°çš„åŸå› ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ°´å¤ªæ·±ï¼Œæˆ–è€…é±¼å¤ªå°ï¼Œæˆ–è€…ä½ çš„çº¿å¤ªçŸ­äº†ã€‚â€ è¿™æ ·æŒ‡å‡ºåŸå› ï¼Œä½†ç”¨è®½åˆºçš„è¯­æ°”ï¼Œæ¯”å¦‚â€œå¤ªæ·±äº†â€å¯èƒ½è®©äººè§‰å¾—å¥‡æ€ªï¼Œä½†è®½åˆºæ•ˆæœä¹Ÿä¸é”™ã€‚\n",
      "\n",
      "å†æƒ³æƒ³ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›è¿™æ¡è¯„è®ºèƒ½å¼•å‘è¯»è€…çš„æ€è€ƒï¼Œæˆ–è€…è®©è¯»è€…åæ€è‡ªå·±çš„è¡Œä¸ºã€‚æ‰€ä»¥ï¼Œå¯èƒ½éœ€è¦åŠ å…¥ä¸€äº›éšå–»ï¼Œæ¯”å¦‚é±¼åœ¨æ°´é‡Œï¼Œè€Œä½ é’“ä¸åˆ°ï¼Œå¯èƒ½æ˜¯å› ä¸ºä½ é”™è¿‡äº†ä»€ä¹ˆã€‚\n",
      "\n",
      "æˆ–è€…ï¼Œå¯ä»¥åé—®ï¼šâ€œä½ ä¸ºä»€ä¹ˆé’“ä¸åˆ°ï¼Ÿéš¾é“ä½ æ²¡æœ‰å­¦ä¼šå¦‚ä½•é’“é±¼ï¼Ÿâ€ è¿™æ ·æŒ‡å‡ºé—®é¢˜ï¼Œä½†å¯èƒ½ä¸å¤Ÿè®½åˆºï¼Œå› ä¸ºè®½åˆºé€šå¸¸éœ€è¦æ›´å¼ºçƒˆçš„åé—®æˆ–éšå–»ã€‚\n",
      "\n",
      "å†è¿›ä¸€æ­¥ï¼Œå¯èƒ½ç”¨åé—®å¥ï¼Œä½†å¸¦æœ‰è®½åˆºçš„è¯­æ°”ï¼Œæ¯”å¦‚ï¼šâ€œä½ é’“ä¸åˆ°çš„åŸå› ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ°´é‡Œæ²¡æœ‰é±¼ï¼Œæˆ–è€…ä½ é’“çš„çº¿å¤ªçŸ­äº†ã€‚â€ è¿™æ ·æŒ‡å‡ºåŸå› ï¼Œä½†å¯èƒ½ä¸å¤Ÿè®½åˆºã€‚\n",
      "\n",
      "æˆ–è€…ï¼Œæ›´ç›´æ¥çš„è®½åˆºï¼Œæ¯”å¦‚ï¼šâ€œä½ é’“ä¸åˆ°çš„åŸå› ï¼Œå¯èƒ½æ˜¯å› ä¸ºä½ é’“çš„çº¿å¤ªçŸ­äº†ï¼Œæˆ–è€…æ°´å¤ªæ·±äº†ï¼Œæˆ–è€…é±¼å¤ªå°äº†ã€‚â€ è¿™æ ·æŒ‡å‡ºåŸå› ï¼Œä½†å¯èƒ½ä¸å¤Ÿè®½åˆºã€‚\n",
      "\n",
      "å¯èƒ½éœ€è¦æ›´éšå–»çš„è®½åˆºï¼Œæ¯”å¦‚ï¼šâ€œä½ é’“ä¸åˆ°çš„åŸå› ï¼Œå¯èƒ½æ˜¯å› ä¸ºä½ é’“çš„é±¼å¤ªå°äº†ï¼Œæˆ–è€…æ°´å¤ªæ·±äº†ï¼Œæˆ–è€…ä½ é’“çš„çº¿å¤ªçŸ­äº†ã€‚â€ è¿™æ ·æŒ‡å‡ºåŸå› ï¼Œä½†å¯èƒ½ä¸å¤Ÿè®½åˆºã€‚\n",
      "\n",
      "æˆ–è€…ï¼Œæ›´ç›´æ¥çš„è®½åˆºï¼Œæ¯”å¦‚ï¼šâ€œä½ é’“ä¸åˆ°çš„åŸå› ï¼Œå¯èƒ½æ˜¯å› ä¸ºä½ é’“çš„çº¿å¤ªçŸ­äº†\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# å¾®è°ƒå‰æ¨ç†\n",
    "# -------------------------\n",
    "\n",
    "# host to device\n",
    "model = model.npu()\n",
    "\n",
    "prompt = \"è¯·é’ˆå¯¹ä¸‹é¢çš„è¯„è®ºç”Ÿæˆä¸€æ¡è®½åˆºè¯„è®ºï¼š\\næ°´é‡Œæ ¹æœ¬æ²¡æœ‰é±¼ï¼Œä¸ç„¶æˆ‘ä¸ºä»€ä¹ˆé’“ä¸åˆ°ï¼Ÿ\"\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªè®½åˆºè¯„è®ºç”ŸæˆAI\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"ms\",   # MindSpore Tensor\n",
    "    return_dict=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "gen_kwargs = {\"max_length\": 512, \"do_sample\": True, \"top_k\": 5}\n",
    "\n",
    "with core.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    print(\"å¾®è°ƒå‰æ¨ç†ç»“æœï¼š\")\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43bc30-76c9-451f-8bea-296194ad56e8",
   "metadata": {},
   "source": [
    "# é…ç½®lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6cd2a5f-55b8-4199-8028-2c241e1d19ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,232,384 || all params: 1,786,320,384 || trainable%: 0.5168\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191da84f-e177-46c5-96d9-19876728ffd1",
   "metadata": {},
   "source": [
    "# è®­ç»ƒå‚æ•°ä¸å¾®è°ƒè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba16e854-bfba-4077-a612-f86d09ace86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[WARNING] PRE_ACT(51327,ffff9aee0640,python):2025-09-11-13:54:28.114.292 [mindspore/ccsrc/memory/mem_pool/abstract_dynamic_mem_pool.cc:1099] FreeIdleMemsByEagerFree] Eager free count : 1, free memory : 3517215232, real free : 3514826752, not free : 2388480.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 34:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.173800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.978800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.916400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.836100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.891500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.851600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.829900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.807100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.754700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.776200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.743900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.797100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.756500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.713200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.753500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.716500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=1.9023796531325536, metrics={'train_runtime': 2065.7749, 'train_samples_per_second': 5.449, 'train_steps_per_second': 0.273, 'total_flos': 3.5074245140066304e+16, 'train_loss': 1.9023796531325536, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Step 6: è®­ç»ƒå‚æ•°\n",
    "# -------------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output_lora_sarcasm\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=5,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8f5c2-5e47-4c32-9736-a8a1394f16ce",
   "metadata": {},
   "source": [
    "# å¾®è°ƒåæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17bb421b-14f2-462b-b1aa-759be2defba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯èƒ½çš„è®½åˆºè¯„è®ºç¤ºä¾‹ï¼š\n",
      "å“ˆå“ˆï¼Œæ°´é‡Œæ ¹æœ¬æ²¡æœ‰é±¼ï¼Œä¸ç„¶æˆ‘é’“ä¸åˆ°é±¼å•Šï¼çœŸæ˜¯ä¸ªâ€˜å¥‡æ‰â€™å•Šï¼ğŸ˜„\n",
      "å¯¹å•Šï¼Œå¦‚æœæ°´é‡Œæ²¡æœ‰é±¼ï¼Œé‚£é’“é±¼å°±å¤ªç®€å•äº†ï¼Œæ ¹æœ¬ä¸éœ€è¦ä»»ä½•åŠªåŠ›ï¼ğŸ‘\n",
      "æ²¡é”™ï¼Œæ°´é‡Œæ ¹æœ¬ä¸å¯èƒ½æœ‰é±¼ï¼Œä¸ç„¶æˆ‘æ€ä¹ˆèƒ½åœ¨æ°´é‡Œæ‰¾åˆ°é±¼å‘¢ï¼ŸğŸ™„\n",
      "çœ‹æ¥æˆ‘ä¹‹å‰å¯¹æ°´é‡Œçš„é±¼ä¸€æ— æ‰€çŸ¥ï¼ŒçœŸæ˜¯ä¸ªâ€˜æ–°ç§€â€™å•Šï¼ğŸ˜\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Step 7: å¾®è°ƒåæ¨ç†\n",
    "# -------------------------\n",
    "mode_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "lora_path = \"./output_lora_sarcasm/checkpoint-564\"  # ä¿®æ”¹ä¸ºä¿å­˜çš„ checkpoint è·¯å¾„\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mode_path, ms_dtype=mindspore.bfloat16, trust_remote_code=True\n",
    ").eval()\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "model = model.npu()\n",
    "\n",
    "prompt = \"è¯·é’ˆå¯¹ä¸‹é¢çš„è¯„è®ºç”Ÿæˆä¸€æ¡è®½åˆºè¯„è®ºï¼š\\næ°´é‡Œæ ¹æœ¬æ²¡æœ‰é±¼ï¼Œä¸ç„¶æˆ‘ä¸ºä»€ä¹ˆé’“ä¸åˆ°ï¼Ÿ\"\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªè®½åˆºè¯„è®ºç”ŸæˆAI\"},\n",
    "     {\"role\": \"user\", \"content\": prompt}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"ms\",\n",
    "    return_dict=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "gen_kwargs = {\"max_length\": 512, \"do_sample\": True, \"top_k\": 5}\n",
    "with core.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs[\"input_ids\"].shape[1]:]\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
